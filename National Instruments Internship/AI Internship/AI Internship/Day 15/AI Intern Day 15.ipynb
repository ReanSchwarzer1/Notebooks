{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import nltk\nnltk.download()","execution_count":1,"outputs":[{"output_type":"stream","text":"NLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> l\nPackages:\n  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n  [ ] comparative_sentences Comparative Sentence Dataset\n  [ ] dolch............... Dolch Word List\n  [ ] framenet_v15........ FrameNet 1.5\n  [ ] framenet_v17........ FrameNet 1.7\n  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n                           2015) subset of the Paraphrase Database.\n  [ ] nombank.1.0......... NomBank Corpus 1.0\n  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n                           Evaluation Shared Task\n  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n                           character properties in Perl\n  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n  [ ] wmt15_eval.......... Evaluation data from WMT15\n\nCollections:\n  [P] all-corpora......... All the corpora\n  [P] all-nltk............ All packages available on nltk_data gh-pages\n                           branch\nHit Enter to continue: all-corpora\n  [P] all................. All packages\n  [P] book................ Everything used in the NLTK Book\n  [P] tests............... Packages for running tests\n  [ ] third-party......... Third-party data packages\n\n([*] marks installed packages; [P] marks partially installed collections)\n\nDownload which package (l=list; x=cancel)?\n  Identifier> all\n    Downloading collection 'all'\n       | \n       | Downloading package abc to /usr/share/nltk_data...\n       |   Package abc is already up-to-date!\n       | Downloading package alpino to /usr/share/nltk_data...\n       |   Package alpino is already up-to-date!\n       | Downloading package biocreative_ppi to\n       |     /usr/share/nltk_data...\n       |   Package biocreative_ppi is already up-to-date!\n       | Downloading package brown to /usr/share/nltk_data...\n       |   Package brown is already up-to-date!\n       | Downloading package brown_tei to /usr/share/nltk_data...\n       |   Package brown_tei is already up-to-date!\n       | Downloading package cess_cat to /usr/share/nltk_data...\n       |   Package cess_cat is already up-to-date!\n       | Downloading package cess_esp to /usr/share/nltk_data...\n       |   Package cess_esp is already up-to-date!\n       | Downloading package chat80 to /usr/share/nltk_data...\n       |   Package chat80 is already up-to-date!\n       | Downloading package city_database to /usr/share/nltk_data...\n       |   Package city_database is already up-to-date!\n       | Downloading package cmudict to /usr/share/nltk_data...\n       |   Package cmudict is already up-to-date!\n       | Downloading package comparative_sentences to\n       |     /usr/share/nltk_data...\n       |   Unzipping corpora/comparative_sentences.zip.\n       | Downloading package comtrans to /usr/share/nltk_data...\n       |   Package comtrans is already up-to-date!\n       | Downloading package conll2000 to /usr/share/nltk_data...\n       |   Package conll2000 is already up-to-date!\n       | Downloading package conll2002 to /usr/share/nltk_data...\n       |   Package conll2002 is already up-to-date!\n       | Downloading package conll2007 to /usr/share/nltk_data...\n       |   Package conll2007 is already up-to-date!\n       | Downloading package crubadan to /usr/share/nltk_data...\n       |   Package crubadan is already up-to-date!\n       | Downloading package dependency_treebank to\n       |     /usr/share/nltk_data...\n       |   Package dependency_treebank is already up-to-date!\n       | Downloading package dolch to /usr/share/nltk_data...\n       |   Unzipping corpora/dolch.zip.\n       | Downloading package europarl_raw to /usr/share/nltk_data...\n       |   Package europarl_raw is already up-to-date!\n       | Downloading package floresta to /usr/share/nltk_data...\n       |   Package floresta is already up-to-date!\n       | Downloading package framenet_v15 to /usr/share/nltk_data...\n       |   Unzipping corpora/framenet_v15.zip.\n       | Downloading package framenet_v17 to /usr/share/nltk_data...\n       |   Unzipping corpora/framenet_v17.zip.\n       | Downloading package gazetteers to /usr/share/nltk_data...\n       |   Package gazetteers is already up-to-date!\n       | Downloading package genesis to /usr/share/nltk_data...\n       |   Package genesis is already up-to-date!\n       | Downloading package gutenberg to /usr/share/nltk_data...\n       |   Package gutenberg is already up-to-date!\n       | Downloading package ieer to /usr/share/nltk_data...\n       |   Package ieer is already up-to-date!\n       | Downloading package inaugural to /usr/share/nltk_data...\n       |   Package inaugural is already up-to-date!\n       | Downloading package indian to /usr/share/nltk_data...\n       |   Package indian is already up-to-date!\n       | Downloading package jeita to /usr/share/nltk_data...\n       |   Package jeita is already up-to-date!\n       | Downloading package kimmo to /usr/share/nltk_data...\n       |   Package kimmo is already up-to-date!\n       | Downloading package knbc to /usr/share/nltk_data...\n       |   Package knbc is already up-to-date!\n       | Downloading package lin_thesaurus to /usr/share/nltk_data...\n       |   Package lin_thesaurus is already up-to-date!\n       | Downloading package mac_morpho to /usr/share/nltk_data...\n       |   Package mac_morpho is already up-to-date!\n       | Downloading package machado to /usr/share/nltk_data...\n       |   Package machado is already up-to-date!\n       | Downloading package masc_tagged to /usr/share/nltk_data...\n       |   Package masc_tagged is already up-to-date!\n       | Downloading package moses_sample to /usr/share/nltk_data...\n       |   Package moses_sample is already up-to-date!\n       | Downloading package movie_reviews to /usr/share/nltk_data...\n       |   Package movie_reviews is already up-to-date!\n       | Downloading package names to /usr/share/nltk_data...\n       |   Package names is already up-to-date!\n       | Downloading package nombank.1.0 to /usr/share/nltk_data...\n       | Downloading package nps_chat to /usr/share/nltk_data...\n       |   Package nps_chat is already up-to-date!\n       | Downloading package omw to /usr/share/nltk_data...\n       |   Package omw is already up-to-date!\n       | Downloading package opinion_lexicon to\n       |     /usr/share/nltk_data...\n       |   Package opinion_lexicon is already up-to-date!\n       | Downloading package paradigms to /usr/share/nltk_data...\n       |   Package paradigms is already up-to-date!\n       | Downloading package pil to /usr/share/nltk_data...\n       |   Package pil is already up-to-date!\n       | Downloading package pl196x to /usr/share/nltk_data...\n       |   Package pl196x is already up-to-date!\n       | Downloading package ppattach to /usr/share/nltk_data...\n       |   Package ppattach is already up-to-date!\n       | Downloading package problem_reports to\n       |     /usr/share/nltk_data...\n       |   Package problem_reports is already up-to-date!\n       | Downloading package propbank to /usr/share/nltk_data...\n       |   Package propbank is already up-to-date!\n       | Downloading package ptb to /usr/share/nltk_data...\n       |   Package ptb is already up-to-date!\n       | Downloading package product_reviews_1 to\n       |     /usr/share/nltk_data...\n       |   Package product_reviews_1 is already up-to-date!\n       | Downloading package product_reviews_2 to\n       |     /usr/share/nltk_data...\n       |   Package product_reviews_2 is already up-to-date!\n       | Downloading package pros_cons to /usr/share/nltk_data...\n       |   Package pros_cons is already up-to-date!\n       | Downloading package qc to /usr/share/nltk_data...\n       |   Package qc is already up-to-date!\n       | Downloading package reuters to /usr/share/nltk_data...\n       |   Package reuters is already up-to-date!\n       | Downloading package rte to /usr/share/nltk_data...\n       |   Package rte is already up-to-date!\n       | Downloading package semcor to /usr/share/nltk_data...\n       |   Package semcor is already up-to-date!\n       | Downloading package senseval to /usr/share/nltk_data...\n       |   Package senseval is already up-to-date!\n       | Downloading package sentiwordnet to /usr/share/nltk_data...\n       |   Package sentiwordnet is already up-to-date!\n       | Downloading package sentence_polarity to\n       |     /usr/share/nltk_data...\n       |   Package sentence_polarity is already up-to-date!\n       | Downloading package shakespeare to /usr/share/nltk_data...\n       |   Package shakespeare is already up-to-date!\n       | Downloading package sinica_treebank to\n       |     /usr/share/nltk_data...\n       |   Package sinica_treebank is already up-to-date!\n       | Downloading package smultron to /usr/share/nltk_data...\n       |   Package smultron is already up-to-date!\n       | Downloading package state_union to /usr/share/nltk_data...\n       |   Package state_union is already up-to-date!\n       | Downloading package stopwords to /usr/share/nltk_data...\n       |   Package stopwords is already up-to-date!\n       | Downloading package subjectivity to /usr/share/nltk_data...\n       |   Package subjectivity is already up-to-date!\n       | Downloading package swadesh to /usr/share/nltk_data...\n       |   Package swadesh is already up-to-date!\n       | Downloading package switchboard to /usr/share/nltk_data...\n       |   Package switchboard is already up-to-date!\n       | Downloading package timit to /usr/share/nltk_data...\n       |   Package timit is already up-to-date!\n       | Downloading package toolbox to /usr/share/nltk_data...\n       |   Package toolbox is already up-to-date!\n       | Downloading package treebank to /usr/share/nltk_data...\n       |   Package treebank is already up-to-date!\n       | Downloading package twitter_samples to\n       |     /usr/share/nltk_data...\n       |   Package twitter_samples is already up-to-date!\n       | Downloading package udhr to /usr/share/nltk_data...\n       |   Package udhr is already up-to-date!\n       | Downloading package udhr2 to /usr/share/nltk_data...\n       |   Package udhr2 is already up-to-date!\n       | Downloading package unicode_samples to\n       |     /usr/share/nltk_data...\n       |   Package unicode_samples is already up-to-date!\n       | Downloading package universal_treebanks_v20 to\n       |     /usr/share/nltk_data...\n       |   Package universal_treebanks_v20 is already up-to-date!\n       | Downloading package verbnet to /usr/share/nltk_data...\n       |   Package verbnet is already up-to-date!\n       | Downloading package verbnet3 to /usr/share/nltk_data...\n       |   Unzipping corpora/verbnet3.zip.\n       | Downloading package webtext to /usr/share/nltk_data...\n       |   Package webtext is already up-to-date!\n       | Downloading package wordnet to /usr/share/nltk_data...\n       |   Package wordnet is already up-to-date!\n       | Downloading package wordnet_ic to /usr/share/nltk_data...\n       |   Package wordnet_ic is already up-to-date!\n       | Downloading package words to /usr/share/nltk_data...\n       |   Package words is already up-to-date!\n       | Downloading package ycoe to /usr/share/nltk_data...\n       |   Package ycoe is already up-to-date!\n       | Downloading package rslp to /usr/share/nltk_data...\n       |   Package rslp is already up-to-date!\n       | Downloading package maxent_treebank_pos_tagger to\n       |     /usr/share/nltk_data...\n       |   Package maxent_treebank_pos_tagger is already up-to-date!\n       | Downloading package universal_tagset to\n       |     /usr/share/nltk_data...\n       |   Package universal_tagset is already up-to-date!\n       | Downloading package maxent_ne_chunker to\n       |     /usr/share/nltk_data...\n       |   Package maxent_ne_chunker is already up-to-date!\n       | Downloading package punkt to /usr/share/nltk_data...\n       |   Package punkt is already up-to-date!\n       | Downloading package book_grammars to /usr/share/nltk_data...\n       |   Package book_grammars is already up-to-date!\n       | Downloading package sample_grammars to\n       |     /usr/share/nltk_data...\n       |   Package sample_grammars is already up-to-date!\n       | Downloading package spanish_grammars to\n       |     /usr/share/nltk_data...\n       |   Package spanish_grammars is already up-to-date!\n       | Downloading package basque_grammars to\n       |     /usr/share/nltk_data...\n       |   Package basque_grammars is already up-to-date!\n       | Downloading package large_grammars to /usr/share/nltk_data...\n       |   Package large_grammars is already up-to-date!\n       | Downloading package tagsets to /usr/share/nltk_data...\n       |   Package tagsets is already up-to-date!\n       | Downloading package snowball_data to /usr/share/nltk_data...\n       |   Package snowball_data is already up-to-date!\n       | Downloading package bllip_wsj_no_aux to\n       |     /usr/share/nltk_data...\n       |   Package bllip_wsj_no_aux is already up-to-date!\n       | Downloading package word2vec_sample to\n       |     /usr/share/nltk_data...\n       |   Package word2vec_sample is already up-to-date!\n       | Downloading package panlex_swadesh to /usr/share/nltk_data...\n","name":"stdout"},{"output_type":"stream","text":"       | Downloading package mte_teip5 to /usr/share/nltk_data...\n       |   Package mte_teip5 is already up-to-date!\n       | Downloading package averaged_perceptron_tagger to\n       |     /usr/share/nltk_data...\n       |   Package averaged_perceptron_tagger is already up-to-date!\n       | Downloading package averaged_perceptron_tagger_ru to\n       |     /usr/share/nltk_data...\n       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n       | Downloading package perluniprops to /usr/share/nltk_data...\n       |   Unzipping misc/perluniprops.zip.\n       | Downloading package nonbreaking_prefixes to\n       |     /usr/share/nltk_data...\n       |   Unzipping corpora/nonbreaking_prefixes.zip.\n       | Downloading package vader_lexicon to /usr/share/nltk_data...\n       |   Package vader_lexicon is already up-to-date!\n       | Downloading package porter_test to /usr/share/nltk_data...\n       |   Package porter_test is already up-to-date!\n       | Downloading package wmt15_eval to /usr/share/nltk_data...\n       |   Unzipping models/wmt15_eval.zip.\n       | Downloading package mwa_ppdb to /usr/share/nltk_data...\n       |   Unzipping misc/mwa_ppdb.zip.\n       | \n     Done downloading collection all\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\n","name":"stdout"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.book import *","execution_count":2,"outputs":[{"output_type":"stream","text":"*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\ntext1: Moby Dick by Herman Melville 1851\ntext2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\ntext4: Inaugural Address Corpus\ntext5: Chat Corpus\ntext6: Monty Python and the Holy Grail\ntext7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(text3)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"44764"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(text3))","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"2789"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text3.count(\"In\")","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"12"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"FreqDist(text3).most_common(50)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[(',', 3681),\n ('and', 2428),\n ('the', 2411),\n ('of', 1358),\n ('.', 1315),\n ('And', 1250),\n ('his', 651),\n ('he', 648),\n ('to', 611),\n (';', 605),\n ('unto', 590),\n ('in', 588),\n ('that', 509),\n ('I', 484),\n ('said', 476),\n ('him', 387),\n ('a', 342),\n ('my', 325),\n ('was', 317),\n ('for', 297),\n ('it', 290),\n ('with', 289),\n ('me', 282),\n ('thou', 272),\n (\"'\", 268),\n ('is', 267),\n ('thy', 267),\n ('s', 263),\n ('thee', 257),\n ('be', 254),\n ('shall', 253),\n ('they', 249),\n ('all', 245),\n (':', 238),\n ('God', 231),\n ('them', 230),\n ('not', 224),\n ('which', 198),\n ('father', 198),\n ('will', 195),\n ('land', 184),\n ('Jacob', 179),\n ('came', 177),\n ('her', 173),\n ('LORD', 166),\n ('were', 163),\n ('she', 161),\n ('from', 157),\n ('Joseph', 157),\n ('their', 153)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text2.collocations()","execution_count":14,"outputs":[{"output_type":"stream","text":"Colonel Brandon; Sir John; Lady Middleton; Miss Dashwood; every thing;\nthousand pounds; dare say; Miss Steeles; said Elinor; Miss Steele;\nevery body; John Dashwood; great deal; Harley Street; Berkeley Street;\nMiss Dashwoods; young man; Combe Magna; every day; next morning\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Tokenization\n\nfrom nltk import word_tokenize\n\nimport nltk, re, pprint","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from urllib import request\nurl = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(raw)","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"1176967"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = word_tokenize(raw)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tokens)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"257727"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens[100:150]","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"['Fyodor',\n 'Dostoevsky',\n 'This',\n 'eBook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"text = nltk.Text(tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bigrm = list(nltk.bigrams(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(*map(' '.join, bigrm), sep=', ')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}